# ğŸ’¬ Talk to Your Data

A production-grade conversational analytics platform that lets you upload spreadsheets and ask questions in natural language. Powered by LangGraph, GPT-4, and pandas.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.41-red.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)
![Redis](https://img.shields.io/badge/Redis-7-red.svg)

---

## âœ¨ Features

### Core Functionality
- **ğŸ“ Multi-file Upload** - Upload CSV and Excel files with automatic schema detection
- **ğŸ’¬ Natural Language Queries** - Ask questions like "What's the average revenue by region?"
- **ğŸ”„ Cross-table Analysis** - Compare data across multiple files and time periods
- **ğŸ“Š Interactive Charts** - Visualize results with Plotly charts
- **ğŸ§  AI-Powered Insights** - Get key findings, recommendations, and actionable insights

### Technical Features
- **ğŸ”— LangGraph Pipeline** - Structured reasoning flow with retry logic
- **âš¡ Redis Caching** - Cache analysis results for instant repeated queries
- **ğŸš¦ Rate Limiting** - Protect against abuse (60 req/min, 1000 req/hr)
- **ğŸ“¦ Background Tasks** - Celery workers for heavy processing
- **ğŸ”Œ Connection Pooling** - Optimized database connections
- **ğŸ“ Structured Logging** - JSON logs for observability

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚â”€â”€â”€â”€â–¶â”‚    FastAPI      â”‚â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
â”‚   Frontend      â”‚     â”‚    Backend      â”‚     â”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   LangGraph     â”‚
                        â”‚   Pipeline      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                  â–¼                  â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   OpenAI    â”‚    â”‚    Redis    â”‚    â”‚   Celery    â”‚
     â”‚   GPT-4     â”‚    â”‚    Cache    â”‚    â”‚   Workers   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LangGraph Pipeline Flow

```
User Query â†’ Ingest â†’ Plan â†’ Generate Code â†’ Validate â†’ Execute â†’ Explain â†’ Response
                â†“                                          â†‘
            [Retry on failure with error context] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Prerequisites

- **Python 3.11+**
- **Docker & Docker Compose** (for PostgreSQL and Redis)
- **OpenAI API Key** (or Anthropic/Ollama)

---

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd pushkal

# Create environment file
cp env.example .env
```

### 2. Configure Environment

Edit `.env` with your settings:

```bash
# Required: LLM API Key
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-4o-mini

# Database (default works with Docker)
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/pushkal_db

# Redis (default works with Docker)
REDIS_URL=redis://localhost:6379/0
```

### 3. Start Infrastructure

```bash
# Start PostgreSQL and Redis
docker-compose up -d

# Verify services are running
docker-compose ps
```

### 4. Setup Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run database migrations
alembic upgrade head

# Start the API server
uvicorn app.main:app --reload --port 8000
```

### 5. Setup Frontend

Open a new terminal:

```bash
cd frontend

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start Streamlit
streamlit run app.py --server.port 8501
```

### 6. Access the Application

- **Frontend**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

---

## ğŸ“– Usage Guide

### Step 1: Upload Data Files

1. Navigate to **ğŸ“ Upload** page
2. Drag and drop your CSV or Excel files
3. The system automatically detects:
   - Column types (numeric, categorical, date)
   - Time periods from filenames (e.g., `sales_nov_2024.csv` â†’ "Nov 2024")

### Step 2: Ask Questions

Navigate to **ğŸ’¬ Chat** and ask questions like:

| Query Type | Example |
|------------|---------|
| **Aggregation** | "What is the average revenue by region?" |
| **Comparison** | "Compare sales between November and December 2024" |
| **Ranking** | "Show top 5 products by units sold" |
| **Filtering** | "Total revenue for APAC region only" |
| **Trends** | "How did revenue change over time?" |

### Step 3: View Results

- **ğŸ“Š Charts** - Interactive Plotly visualizations
- **ğŸ“‹ Tables** - Detailed data tables
- **ğŸ’¡ Insights** - AI-generated key findings and recommendations

### Step 4: Review History

Navigate to **ğŸ“Š Analysis** to:
- View past analyses
- Download generated Python code
- Export results as CSV

---

## ğŸ”§ Configuration Options

### LLM Providers

The application supports multiple LLM providers. Configure in `.env`:

#### OpenAI (Recommended)
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-key
OPENAI_MODEL=gpt-4o-mini  # or gpt-4o, gpt-4-turbo
```

#### Anthropic Claude
```bash
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-your-key
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

#### Ollama (Local)
```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

### Rate Limiting

Configure in `backend/app/main.py`:
```python
app.add_middleware(
    RateLimitMiddleware,
    requests_per_minute=60,
    requests_per_hour=1000,
)
```

### Cache TTL

Configure in `backend/app/core/cache.py`:
```python
FILE_METADATA_TTL = 7200      # 2 hours
ANALYSIS_RESULT_TTL = 1800    # 30 minutes
SESSION_TTL = 86400           # 24 hours
```

---

## ğŸ“ Project Structure

```
pushkal/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/               # API Routes
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py      # File upload endpoints
â”‚   â”‚   â”‚   â””â”€â”€ chat.py        # Chat endpoints
â”‚   â”‚   â”œâ”€â”€ core/              # Core utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Settings management
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py       # Redis caching
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware.py  # Rate limiting
â”‚   â”‚   â”‚   â””â”€â”€ celery_app.py  # Task queue
â”‚   â”‚   â”œâ”€â”€ models/            # SQLAlchemy Models
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py     # Session model
â”‚   â”‚   â”‚   â”œâ”€â”€ file.py        # UploadedFile model
â”‚   â”‚   â”‚   â”œâ”€â”€ message.py     # ChatMessage model
â”‚   â”‚   â”‚   â””â”€â”€ analysis.py    # AnalysisResult model
â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic Schemas
â”‚   â”‚   â”œâ”€â”€ services/          # Business Logic
â”‚   â”‚   â”‚   â”œâ”€â”€ file_service.py
â”‚   â”‚   â”‚   â””â”€â”€ chat_service.py
â”‚   â”‚   â”œâ”€â”€ tasks/             # Celery Tasks
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py    # Background analysis
â”‚   â”‚   â”‚   â””â”€â”€ cleanup.py     # Maintenance tasks
â”‚   â”‚   â””â”€â”€ main.py            # Application entry
â”‚   â”œâ”€â”€ alembic/               # Database migrations
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ alembic.ini
â”‚
â”œâ”€â”€ frontend/                   # Streamlit Frontend
â”‚   â”œâ”€â”€ app.py                 # Main application
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_ğŸ“_Upload.py     # Upload page
â”‚   â”‚   â”œâ”€â”€ 2_ğŸ’¬_Chat.py       # Chat page
â”‚   â”‚   â””â”€â”€ 3_ğŸ“Š_Analysis.py   # History page
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ pipeline/                   # LangGraph Pipeline
â”‚   â”œâ”€â”€ graph.py               # State graph definition
â”‚   â”œâ”€â”€ state.py               # GraphState TypedDict
â”‚   â”œâ”€â”€ llm.py                 # LLM configuration
â”‚   â””â”€â”€ nodes/                 # Pipeline nodes
â”‚       â”œâ”€â”€ ingest.py          # Query ingestion
â”‚       â”œâ”€â”€ planning.py        # Intent analysis
â”‚       â”œâ”€â”€ code.py            # Code generation
â”‚       â””â”€â”€ explain.py         # Result explanation
â”‚
â”œâ”€â”€ data/                       # Sample data files
â”‚   â”œâ”€â”€ sales_nov_2024.csv
â”‚   â”œâ”€â”€ sales_dec_2024.csv
â”‚   â””â”€â”€ sales_q1_2025.csv
â”‚
â”œâ”€â”€ docker-compose.yml          # Infrastructure
â”œâ”€â”€ .env                        # Environment config
â””â”€â”€ README.md
```

---

## ğŸ”Œ API Reference

### Health Check
```bash
GET /health
```
Returns status of all services (API, Database, Redis).

### Upload File
```bash
POST /api/v1/upload
Content-Type: multipart/form-data

file: <file>
session_id: <string>
```

### List Files
```bash
GET /api/v1/files?session_id=<session_id>
```

### Send Chat Message
```bash
POST /api/v1/chat
Content-Type: application/json

{
  "session_id": "your-session-id",
  "message": "What is the average revenue?"
}
```

### Get Chat History
```bash
GET /api/v1/chat/history?session_id=<session_id>&limit=20
```

### Get Analysis Details
```bash
GET /api/v1/chat/analysis/{analysis_id}
```

---

## ğŸ§ª Testing

### Test File Upload
```bash
curl -X POST http://localhost:8000/api/v1/upload \
  -F "file=@data/sales_nov_2024.csv" \
  -F "session_id=test-session"
```

### Test Chat
```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id": "test-session", "message": "What is the total revenue?"}'
```

### Run Unit Tests
```bash
cd backend
pytest tests/ -v
```

---

## ğŸš€ Production Deployment

### Using Docker Compose (Full Stack)

```bash
# Build and start all services
docker-compose -f docker-compose.prod.yml up -d

# View logs
docker-compose logs -f
```

### Using Gunicorn (Backend)

```bash
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

### Start Celery Workers

```bash
# Start worker
celery -A app.core.celery_app worker --loglevel=info -Q analysis,cleanup

# Start beat scheduler (for periodic tasks)
celery -A app.core.celery_app beat --loglevel=info
```

### Environment Variables for Production

```bash
# Security
SECRET_KEY=your-production-secret-key

# Database
DATABASE_URL=postgresql://user:pass@db-host:5432/pushkal_db

# Redis
REDIS_URL=redis://redis-host:6379/0

# Logging
LOG_LEVEL=INFO
```

---

## ğŸ› ï¸ Development

### Adding New LangGraph Nodes

1. Create node function in `pipeline/nodes/`
2. Update state in `pipeline/state.py` if needed
3. Register node in `pipeline/graph.py`

### Database Migrations

```bash
cd backend

# Create new migration
alembic revision --autogenerate -m "Description"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1
```

### Adding New API Endpoints

1. Create router in `backend/app/api/`
2. Add schemas in `backend/app/schemas/`
3. Register router in `backend/app/main.py`

---

## ğŸ› Troubleshooting

### Common Issues

**"Cannot connect to PostgreSQL"**
```bash
# Check if Docker is running
docker-compose ps

# Restart PostgreSQL
docker-compose restart postgres
```

**"OPENAI_API_KEY not found"**
```bash
# Ensure .env file exists and has the key
cat .env | grep OPENAI
```

**"Redis connection failed"**
```bash
# Check Redis status
docker-compose logs redis

# The app works without Redis (caching disabled)
```

**"Module not found: pipeline"**
```bash
# Ensure you're in the project root
cd /path/to/pushkal
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

---

## ğŸ“ Sample Data

The `data/` folder contains sample CSV files for testing:

| File | Description | Rows | Period |
|------|-------------|------|--------|
| `sales_nov_2024.csv` | November sales | 10 | Nov 2024 |
| `sales_dec_2024.csv` | December sales | 12 | Dec 2024 |
| `sales_q1_2025.csv` | Q1 summary | 9 | Q1 2025 |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [LangGraph](https://github.com/langchain-ai/langgraph) - Pipeline orchestration
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [Streamlit](https://streamlit.io/) - Frontend framework
- [OpenAI](https://openai.com/) - LLM provider

---

**Built with â¤ï¸ for conversational data analytics**
